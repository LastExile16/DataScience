{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 10:30:10.780223: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 599 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 3g.20gb, pci bus id: 0000:92:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "mnist_dataset, mnist_info = tfds.load(name='mnist', with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train, mnist_test = mnist_dataset['train'], mnist_dataset['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_validation_samples = 0.1 * mnist_info.splits['train'].num_examples\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64)\n",
    "\n",
    "# num_train_samples = 0.9 * mnist_info.splits['train'].num_examples\n",
    "# num_train_samples = tf.cast(num_train_samples, tf.int64)\n",
    "\n",
    "num_test_samples = mnist_info.splits['test'].num_examples\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(mnist_info.splits['train'].num_examples)\n",
    "print(mnist_info.splits['test'].num_examples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### custom scaler function that takes an input and label, returns both input and label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image, label):\n",
    "    print((tf.math.reduce_max(image)))\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    print((tf.reduce_max(image)))\n",
    "    image /= 255\n",
    "    print((tf.math.reduce_max(image)))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.take_op._TakeDataset'>\n",
      "<class 'tuple'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 10:30:12.553828: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-31 10:30:12.554689: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_4' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_4}}]]\n",
      "2023-05-31 10:30:12.637628: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "# tf.math.reduce_mean(mnist_train['image'])\n",
    "# the dataset consists of a number of lists\n",
    "# each list consist of a multidimentional matrix according to the dataset\n",
    "# in this case each tensor is of size 1x28x28\n",
    "ds = mnist_train.take(1)\n",
    "print(type(ds))\n",
    "\n",
    "for example in (ds):\n",
    "    print(type(example))\n",
    "    # print(x.reshape(1,28,28))\n",
    "    # print(type(tf.convert_to_tensor(x)))\n",
    "    # print(example[0][27])\n",
    "    # print(tf.reduce_max(example))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tensorflow provides _map_ function to use a custom scaler that you want\n",
    "* I'm not sure why we have to standardize the values for pixels where it already has a fixed range of 0-255. that means it is already scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Max:0\", shape=(), dtype=uint8)\n",
      "Tensor(\"Max_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Max_2:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Max:0\", shape=(), dtype=uint8)\n",
      "Tensor(\"Max_1:0\", shape=(), dtype=float32)\n",
      "Tensor(\"Max_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "scaled_train_and_validation_data = mnist_train.map(scale)\n",
    "test_data = mnist_test.map(scale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the training data to prepare for random split (to extract validation data)\n",
    "* BUFFER_SIZE is used to ask the shuffle function to take this bunch of data at a time and shuffle.\n",
    "* if BUFFER_SIZE is 1, no shuffle will happen.\n",
    "* if BUFFER_SIZE is full sample, the shuffle will upload all the data into the memory then shuffle all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "shuffled_train_and_validation_data = scaled_train_and_validation_data.shuffle(BUFFER_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split training dataset into validation and training with _take(number)_ and _skip(number)_ functions\n",
    "* **data.take(number)**: returns the first _number_ of the dataset.\n",
    "* **data.skip(number)**: returns the dataset after first _number_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = shuffled_train_and_validation_data.take(num_validation_samples).cache()\n",
    "validation_data = validation_data.prefetch(tf.data.AUTOTUNE)\n",
    "validation_data = validation_data.repeat()\n",
    "training_data = shuffled_train_and_validation_data.skip(num_validation_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch the data\n",
    "* Only training data is batched becuase the validation data is used in forward propagation only, thus no weight update is happening and it is relatively small\n",
    "* The tf.data.Dataset.batch() function in TensorFlow returns a batched dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.repeat_op._RepeatDataset"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = training_data.batch(BATCH_SIZE)\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.batch_op._BatchDataset"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(validation_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare validation data to be used by the model\n",
    "* validation_data is of type BatchDataset and iter converts it to a tensor, which can be passed to the fit function\n",
    "* iter() is a python function to change the datatype into an iterable\n",
    "* next() will take the next batch from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 10:30:15.749406: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "2023-05-31 10:30:15.750511: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2023-05-31 10:30:16.490563: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "validation_input, validation_target = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### outline the model\n",
    "- Input:\n",
    "    * each image is 28*28 pixles --> 784 pixels in total\n",
    "    * each pixel can have a value from 0 - 255\n",
    "    * the input data has only one feature which is the greyscale intensity so the data is in 784x1 shape\n",
    "- Output:\n",
    "    * the output layer should have 10 nodes that is from 0 - 9\n",
    "- Hidden Layers:\n",
    "    * we want to create two hidden layers in between to process the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### define the hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100 # this number can be changed as you like."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build a model sequentially\n",
    "* Flatten() function will take the input and convert it from a tensor (of rank3 shape in our example) into the proper vector shape\n",
    "* Dense() function will take the inputs and dot product it with the weights matrix and also the activation function\n",
    "    * thus, it is used to add layers to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size*2, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size/2, activation='tanh'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size/4, activation='tanh'),\n",
    "                            tf.keras.layers.Dense(round(hidden_layer_size/8), activation='tanh'),\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])\n",
    "model2 = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='tanh'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size/2, activation='relu'),\n",
    "                            # tf.keras.layers.Dropout(0.2), # droupout used to prevent overfitting.\n",
    "                            tf.keras.layers.Dense(hidden_layer_size/4, activation='relu'),\n",
    "                            tf.keras.layers.Dense(round(hidden_layer_size/8), activation='relu'),\n",
    "                            tf.keras.layers.Dropout(0.2), # droupout used to prevent overfitting.\n",
    "                            tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "                            ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the optimizer and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new ADAM doesn't work for me!\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "custom_optimizer = tf.keras.optimizers.legacy.Adam()\n",
    "model.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model2.compile(optimizer=custom_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 10:30:18.643299: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_2}}]]\n",
      "2023-05-31 10:30:18.644399: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_3' with dtype int64 and shape [1]\n",
      "\t [[{{node Placeholder/_3}}]]\n",
      "2023-05-31 10:30:20.361125: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:219] failed to create cublas handle: cublasGetStatusString symbol not found.\n",
      "2023-05-31 10:30:20.361209: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:222] Failure to initialize cublas may be due to OOM (cublas needs some free memory when you initialize it, and your deep-learning framework may have preallocated more than its fair share), or may be because this binary was not built with support for the GPU in your machine.\n",
      "2023-05-31 10:30:20.361242: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at matmul_op_impl.h:621 : INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "2023-05-31 10:30:20.361314: I tensorflow/core/common_runtime/executor.cc:1197] [/job:localhost/replica:0/task:0/device:GPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INTERNAL: Attempting to perform BLAS operation using StreamExecutor without BLAS support\n",
      "\t [[{{node sequential/dense/MatMul}}]]\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential/dense/MatMul' defined at (most recent call last):\n    File \"/usr/local/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/local/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/local/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/local/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 423, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_368614/356648397.py\", line 3, in <module>\n      model.fit(training_data, epochs=NUM_EPOCHS, validation_data=(validation_input, validation_target), callbacks=[early_stopping], verbose=2)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/layers/core/dense.py\", line 241, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'sequential/dense/MatMul'\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node sequential/dense/MatMul}}]] [Op:__inference_train_function_1318]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#'loss','accuracy','val_loss','val_accuracy'.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m early_stopping \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mval_accuracy\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model\u001b[39m.\u001b[39;49mfit(training_data, epochs\u001b[39m=\u001b[39;49mNUM_EPOCHS, validation_data\u001b[39m=\u001b[39;49m(validation_input, validation_target), callbacks\u001b[39m=\u001b[39;49m[early_stopping], verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInternalError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential/dense/MatMul' defined at (most recent call last):\n    File \"/usr/local/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/local/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/local/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/local/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/local/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 423, in do_execute\n      res = shell.run_cell(\n    File \"/usr/local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2945, in run_cell\n      result = self._run_cell(\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3000, in _run_cell\n      return runner(coro)\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3203, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3382, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/usr/local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3442, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_368614/356648397.py\", line 3, in <module>\n      model.fit(training_data, epochs=NUM_EPOCHS, validation_data=(validation_input, validation_target), callbacks=[early_stopping], verbose=2)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1685, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1284, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1268, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1249, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1050, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 558, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/sequential.py\", line 412, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/functional.py\", line 512, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/functional.py\", line 669, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/base_layer.py\", line 1145, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/layers/core/dense.py\", line 241, in call\n      outputs = tf.matmul(a=inputs, b=self.kernel)\nNode: 'sequential/dense/MatMul'\nAttempting to perform BLAS operation using StreamExecutor without BLAS support\n\t [[{{node sequential/dense/MatMul}}]] [Op:__inference_train_function_1318]"
     ]
    }
   ],
   "source": [
    "#'loss','accuracy','val_loss','val_accuracy'.\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2)\n",
    "model.fit(training_data, epochs=NUM_EPOCHS, validation_data=(validation_input, validation_target), callbacks=[early_stopping], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "540/540 - 3s - loss: 0.7052 - accuracy: 0.7756 - val_loss: 0.2669 - val_accuracy: 0.9257 - 3s/epoch - 6ms/step\n",
      "Epoch 2/25\n",
      "540/540 - 3s - loss: 0.3246 - accuracy: 0.9017 - val_loss: 0.1569 - val_accuracy: 0.9567 - 3s/epoch - 5ms/step\n",
      "Epoch 3/25\n",
      "540/540 - 3s - loss: 0.2482 - accuracy: 0.9267 - val_loss: 0.1211 - val_accuracy: 0.9678 - 3s/epoch - 5ms/step\n",
      "Epoch 4/25\n",
      "540/540 - 3s - loss: 0.2132 - accuracy: 0.9369 - val_loss: 0.1047 - val_accuracy: 0.9718 - 3s/epoch - 5ms/step\n",
      "Epoch 5/25\n",
      "540/540 - 3s - loss: 0.1816 - accuracy: 0.9454 - val_loss: 0.0913 - val_accuracy: 0.9760 - 3s/epoch - 5ms/step\n",
      "Epoch 6/25\n",
      "540/540 - 3s - loss: 0.1572 - accuracy: 0.9534 - val_loss: 0.0805 - val_accuracy: 0.9765 - 3s/epoch - 5ms/step\n",
      "Epoch 7/25\n",
      "540/540 - 3s - loss: 0.1452 - accuracy: 0.9577 - val_loss: 0.0710 - val_accuracy: 0.9802 - 3s/epoch - 6ms/step\n",
      "Epoch 8/25\n",
      "540/540 - 3s - loss: 0.1258 - accuracy: 0.9649 - val_loss: 0.0625 - val_accuracy: 0.9827 - 3s/epoch - 5ms/step\n",
      "Epoch 9/25\n",
      "540/540 - 3s - loss: 0.1038 - accuracy: 0.9702 - val_loss: 0.0628 - val_accuracy: 0.9827 - 3s/epoch - 5ms/step\n",
      "Epoch 10/25\n",
      "540/540 - 3s - loss: 0.1006 - accuracy: 0.9718 - val_loss: 0.0626 - val_accuracy: 0.9823 - 3s/epoch - 5ms/step\n",
      "Epoch 11/25\n",
      "540/540 - 3s - loss: 0.0928 - accuracy: 0.9736 - val_loss: 0.0582 - val_accuracy: 0.9835 - 3s/epoch - 5ms/step\n",
      "Epoch 12/25\n",
      "540/540 - 3s - loss: 0.0858 - accuracy: 0.9757 - val_loss: 0.0423 - val_accuracy: 0.9880 - 3s/epoch - 5ms/step\n",
      "Epoch 13/25\n",
      "540/540 - 3s - loss: 0.0792 - accuracy: 0.9767 - val_loss: 0.0555 - val_accuracy: 0.9842 - 3s/epoch - 5ms/step\n",
      "Epoch 14/25\n",
      "540/540 - 3s - loss: 0.0749 - accuracy: 0.9784 - val_loss: 0.0546 - val_accuracy: 0.9848 - 3s/epoch - 5ms/step\n",
      "Epoch 15/25\n",
      "540/540 - 3s - loss: 0.0758 - accuracy: 0.9776 - val_loss: 0.0414 - val_accuracy: 0.9888 - 3s/epoch - 5ms/step\n",
      "Epoch 16/25\n",
      "540/540 - 3s - loss: 0.0687 - accuracy: 0.9795 - val_loss: 0.0358 - val_accuracy: 0.9895 - 3s/epoch - 5ms/step\n",
      "Epoch 17/25\n",
      "540/540 - 3s - loss: 0.0690 - accuracy: 0.9808 - val_loss: 0.0292 - val_accuracy: 0.9913 - 3s/epoch - 5ms/step\n",
      "Epoch 18/25\n",
      "540/540 - 3s - loss: 0.0577 - accuracy: 0.9832 - val_loss: 0.0294 - val_accuracy: 0.9917 - 3s/epoch - 5ms/step\n",
      "Epoch 19/25\n",
      "540/540 - 3s - loss: 0.0612 - accuracy: 0.9823 - val_loss: 0.0327 - val_accuracy: 0.9905 - 3s/epoch - 5ms/step\n",
      "Epoch 20/25\n",
      "540/540 - 3s - loss: 0.0617 - accuracy: 0.9823 - val_loss: 0.0336 - val_accuracy: 0.9908 - 3s/epoch - 5ms/step\n",
      "Epoch 21/25\n",
      "540/540 - 3s - loss: 0.0547 - accuracy: 0.9837 - val_loss: 0.0256 - val_accuracy: 0.9937 - 3s/epoch - 5ms/step\n",
      "Epoch 22/25\n",
      "540/540 - 3s - loss: 0.0547 - accuracy: 0.9843 - val_loss: 0.0388 - val_accuracy: 0.9903 - 3s/epoch - 5ms/step\n",
      "Epoch 23/25\n",
      "540/540 - 3s - loss: 0.0521 - accuracy: 0.9850 - val_loss: 0.0327 - val_accuracy: 0.9902 - 3s/epoch - 5ms/step\n",
      "Epoch 24/25\n",
      "540/540 - 3s - loss: 0.0497 - accuracy: 0.9867 - val_loss: 0.0294 - val_accuracy: 0.9913 - 3s/epoch - 5ms/step\n",
      "Epoch 25/25\n",
      "540/540 - 3s - loss: 0.0538 - accuracy: 0.9849 - val_loss: 0.0313 - val_accuracy: 0.9905 - 3s/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb0b030cf40>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(training_data, epochs=NUM_EPOCHS, validation_data=(validation_input, validation_target), verbose=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 15:09:21.226274: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-05-29 15:09:21.227216: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_2' with dtype string and shape [1]\n",
      "\t [[{{node Placeholder/_2}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 457ms/step - loss: 0.0829 - accuracy: 0.9819\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.1598 - accuracy: 0.9738\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)\n",
    "test_loss2, test_accuracy2 = model2.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08, Test accuracy: 98.19%\n",
      "Test loss 2: 0.16, Test accuracy 2: 97.38%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}, Test accuracy: {1:.2f}%'. format(test_loss, test_accuracy*100.))\n",
    "print('Test loss 2: {0:.2f}, Test accuracy 2: {1:.2f}%'. format(test_loss2, test_accuracy2*100.))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
